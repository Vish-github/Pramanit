@article{tzanetakis_musical_2002,
	title = {Musical genre classification of audio signals},
	volume = {10},
	issn = {1063-6676, 1558-2353},
	url = {https://ieeexplore.ieee.org/document/1021072/},
	doi = {10.1109/TSA.2002.800560},
	number = {5},
	urldate = {2022-03-13},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Tzanetakis, G. and Cook, P.},
	month = jul,
	year = {2002},
	pages = {293--302},
}

@misc{gtzan_dataset,
	url = {http://marsyas.info/downloads/datasets.html},
	urldate = {2022-03-13},
}

@article{nanni_ensemble_2018,
	title = {Ensemble of deep learning, visual and acoustic features for music genre classification},
	volume = {47},
	issn = {0929-8215, 1744-5027},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2018.1438476},
	doi = {10.1080/09298215.2018.1438476},
	language = {en},
	number = {4},
	urldate = {2022-03-13},
	journal = {Journal of New Music Research},
	author = {Nanni, Loris and Costa, Yandre M. G. and Aguiar, Rafael L. and Silla, Carlos N. and Brahnam, Sheryl},
	month = aug,
	year = {2018},
	pages = {383--397},
}

@article{ceylan_automatic_2021,
	title = {Automatic {Music} {Genre} {Classification} and {Its} {Relation} with {Music} {Education}},
	volume = {11},
	issn = {1925-0754, 1925-0746},
	url = {http://www.sciedupress.com/journal/index.php/wje/article/view/20421},
	doi = {10.5430/wje.v11n2p36},
	abstract = {Because the classification saves time in the learning process and enables this process to take place more easily, its contribution to music learning cannot be denied. One of the most valid and effective methods in music classification is music genre classification. Given the rapid progress of music production in the world and the significant increase in the number of data, the process of classifying music genres has now become too complex to be done by humans. Considering the successful results of deep neural networks in this field, the aim is to develop a deep learning algorithm that can classify 10 different music genres. To reveal the efficiency of the model by comparing it with others, we make the classification using the GTZAN dataset, which was previously used in many studies and retains its validity. In this article, we use a convolutional neural network (CNN) to classify music genres, taking into account the previous successful results. Unlike previous studies in which CNN was used as a classifier, we represent music segments in the dataset by mel frequency cepstral coefficients (MFCC) instead of using visual features or representations. We obtain MFCCs by preprocessing the music pieces in the dataset, then train a CNN model with the acquired MFCCs and determine the success of the model with the testing data. As a result of this study, we develop a model that is successful in classifying music genres by using smaller data than previous studies.},
	number = {2},
	urldate = {2022-03-13},
	journal = {World Journal of Education},
	author = {Ceylan, Hasan Can and Hardalaç, Naciye and Kara, Ali Can and Hardalaç, Fırat},
	month = apr,
	year = {2021},
	pages = {36},
}

@inproceedings{emotion_classification_datasets,
    title = "An Analysis of Annotated Corpora for Emotion Classification in Text",
    author = "Bostan, Laura-Ana-Maria  and
      Klinger, Roman",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1179",
    pages = "2104--2119",
    abstract = "Several datasets have been annotated and published for classification of emotions. They differ in several ways: (1) the use of different annotation schemata (e. g., discrete label sets, including joy, anger, fear, or sadness or continuous values including valence, or arousal), (2) the domain, and, (3) the file formats. This leads to several research gaps: supervised models often only use a limited set of available resources. Additionally, no previous work has compared emotion corpora in a systematic manner. We aim at contributing to this situation with a survey of the datasets, and aggregate them in a common file format with a common annotation schema. Based on this aggregation, we perform the first cross-corpus classification experiments in the spirit of future research enabled by this paper, in order to gain insight and a better understanding of differences of models inferred from the data. This work also simplifies the choice of the most appropriate resources for developing a model for a novel domain. One result from our analysis is that a subset of corpora is better classified with models trained on a different corpus. For none of the corpora, training on all data altogether is better than using a subselection of the resources. Our unified corpus is available at http://www.ims.uni-stuttgart.de/data/unifyemotion.",
}

@article{ekman_pan-cultural_1969,
	title = {Pan-{Cultural} {Elements} in {Facial} {Displays} of {Emotion}},
	volume = {164},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.164.3875.86},
	doi = {10.1126/science.164.3875.86},
	language = {en},
	number = {3875},
	urldate = {2022-03-13},
	journal = {Science},
	author = {Ekman, Paul and Sorenson, E. Richard and Friesen, Wallace V.},
	month = apr,
	year = {1969},
	pages = {86--88},
}


@misc{transfer_learning,
	title = {Transfer {Learning} {\textbar} {Understanding} {Transfer} {Learning} for {Deep} {Learning}},
	url = {https://www.analyticsvidhya.com/blog/2021/10/understanding-transfer-learning-for-deep-learning/},
	abstract = {The reuse of a pre-trained model on a new problem is called the transfer learning in machine learning and deep learning},
	language = {en},
	urldate = {2022-03-13},
	journal = {Analytics Vidhya},
	month = oct,
	year = {2021},
}
@inproceedings{mel-scale,
author = {Umesh, S. and Cohen, Leon and Nelson, Douglas},
year = {1999},
month = {04},
pages = {217 - 220 vol.1},
title = {Fitting the Mel scale},
volume = {1},
isbn = {0-7803-5041-3},
doi = {10.1109/ICASSP.1999.758101}
}
@misc{mel-frequency_2021,
	title = {Mel-frequency cepstrum},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Mel-frequency_cepstrum&oldid=1021923846},
	abstract = {In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a  linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.
Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear "spectrum-of-a-spectrum"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal spectrum. This frequency warping can allow for better representation of sound, for example, in audio compression.
MFCCs are commonly derived as follows:
Take the Fourier transform of (a windowed excerpt of) a signal.
Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.
Take the logs of the powers at each of the mel frequencies.
Take the discrete cosine transform of the list of mel log powers, as if it were a signal.
The MFCCs are the amplitudes of the resulting spectrum.There can be variations on this process, for example: differences in the shape or spacing of the windows used to map the scale, or addition of dynamics features such as "delta" and "delta-delta" (first- and second-order frame-to-frame difference) coefficients.The European Telecommunications Standards Institute in the early 2000s defined a standardised MFCC algorithm to be used in mobile phones.},
	language = {en},
	urldate = {2022-03-13},
	journal = {Wikipedia},
	month = may,
	year = {2021},
	note = {Page Version ID: 1021923846},
}

@article{mfcc_speaker_2012,
	title = {Design, analysis and experimental evaluation of block based transformation in {MFCC} computation for speaker recognition},
	volume = {54},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639311001622},
	doi = {10.1016/j.specom.2011.11.004},
	language = {en},
	number = {4},
	urldate = {2022-03-13},
	journal = {Speech Communication},
	author = {Sahidullah, Md. and Saha, Goutam},
	month = may,
	year = {2012},
	pages = {543--565},
}

@incollection{spectral_analysis_2008,
	address = {New York, NY},
	title = {Spectral {Analysis} and {Correlation}},
	isbn = {9780387776989 9780387304410},
	url = {http://link.springer.com/10.1007/978-0-387-30441-0_3},
	language = {en},
	urldate = {2022-03-13},
	booktitle = {Handbook of {Signal} {Processing} in {Acoustics}},
	publisher = {Springer New York},
	author = {Randall, Robert B.},
	editor = {Havelock, David and Kuwano, Sonoko and Vorländer, Michael},
	year = {2008},
	doi = {10.1007/978-0-387-30441-0_3},
	pages = {33--52},
}

@Inbook{plutchik_emotions,
author="Imbir, Kamil K.",
editor="Zeigler-Hill, Virgil
and Shackelford, Todd K.",
title="Psychoevolutionary Theory of Emotion (Plutchik)",
bookTitle="Encyclopedia of Personality and Individual Differences",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="1--9",
isbn="978-3-319-28099-8",
doi="10.1007/978-3-319-28099-8_547-1",
url="https://doi.org/10.1007/978-3-319-28099-8_547-1"
}

@inproceedings{ohman-etal-2020-xed,
    title = "{XED}: A Multilingual Dataset for Sentiment Analysis and Emotion Detection",
    author = {{\"O}hman, Emily  and
      P{\`a}mies, Marc  and
      Kajava, Kaisla  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.575",
    doi = "10.18653/v1/2020.coling-main.575",
    pages = "6542--6552",
    abstract = "We introduce XED, a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. We use Plutchik{'}s core emotions to annotate the dataset with the addition of neutral to create a multilabel multiclass dataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to show that XED performs on par with other similar datasets and is therefore a useful tool for sentiment analysis and emotion detection.",
}

@misc{webscraping,
author = {Sharma, Rizul},
year = {2020},
month = {03},
pages = {},
title = {Web Data Scraping},
doi = {10.13140/RG.2.2.15125.76009}
}

@article{emotion_models,
author = {P S, SREEJA and G S, Mahalakshmi},
year = {2017},
month = {01},
pages = {651-657},
title = {Emotion Models: A Review},
volume = {10},
journal = {International Journal of Control Theory and Applications}
}

@inproceedings{opensubtitles2016,
    title = "{O}pen{S}ubtitles2016: Extracting Large Parallel Corpora from Movie and {TV} Subtitles",
    author = {Lison, Pierre  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1147",
    pages = "923--929",
    abstract = "We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.",
}
@inproceedings{rnn-sentiment,
  title={Sentiment Analysis on Twitter using Neural Network: Indonesian Presidential Election 2019 Dataset},
  author={Hidayatullah, Ahmad Fathan and Cahyaningtyas, Siwi and Hakim, Anisa Miladya},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={1077},
  number={1},
  pages={012001},
  year={2021},
  organization={IOP Publishing}
}
@article{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
